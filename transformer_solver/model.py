# transformer_solver/model.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
from tensordict import TensorDict
from dataclasses import dataclass

from .definitions import FEATURE_DIM, FEATURE_INDEX, NODE_TYPE_BATTERY, NODE_TYPE_IC, NODE_TYPE_LOAD
from .utils.common import batchify
from .solver_env import PocatEnv, BATTERY_NODE_IDX

# ğŸ’¡ [CaDA ì¥ì  ì ìš© 1] PrecomputedCache í´ë˜ìŠ¤ ì¶”ê°€
@dataclass
class PrecomputedCache:
    node_embeddings: torch.Tensor
    glimpse_key: torch.Tensor
    glimpse_val: torch.Tensor
    logit_key: torch.Tensor

    def batchify(self, num_starts: int):
        return PrecomputedCache(
            batchify(self.node_embeddings, num_starts),
            batchify(self.glimpse_key, num_starts),
            batchify(self.glimpse_val, num_starts),
            batchify(self.logit_key, num_starts),
        )

# ... (RMSNorm, Normalization, ParallelGatedMLP, FeedForward, reshape_by_headsëŠ” ì´ì „ê³¼ ë™ì¼) ...
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight

class Normalization(nn.Module):
    def __init__(self, embedding_dim, norm_type='rms', **kwargs):
        super().__init__()
        self.norm_type = norm_type
        if self.norm_type == 'layer': self.norm = nn.LayerNorm(embedding_dim)
        elif self.norm_type == 'rms': self.norm = RMSNorm(embedding_dim)
        elif self.norm_type == 'instance': self.norm = nn.InstanceNorm1d(embedding_dim, affine=True, track_running_stats=False)
        else: raise NotImplementedError
    def forward(self, x):
        if self.norm_type == 'instance': return self.norm(x.transpose(1, 2)).transpose(1, 2)
        else: return self.norm(x)

class ParallelGatedMLP(nn.Module):
    def __init__(self, hidden_size: int, **kwargs):
        super().__init__()
        inner_size = int(2 * hidden_size * 4 / 3)
        multiple_of = 256
        inner_size = multiple_of * ((inner_size + multiple_of - 1) // multiple_of)
        self.l1, self.l2, self.l3 = nn.Linear(hidden_size, inner_size, bias=False), nn.Linear(hidden_size, inner_size, bias=False), nn.Linear(inner_size, hidden_size, bias=False)
        self.act = F.silu
    def forward(self, z):
        z1, z2 = self.l1(z), self.l2(z)
        return self.l3(self.act(z1) * z2)

class FeedForward(nn.Module):
    def __init__(self, embedding_dim, ff_hidden_dim, **kwargs):
        super().__init__()
        self.W1 = nn.Linear(embedding_dim, ff_hidden_dim)
        self.W2 = nn.Linear(ff_hidden_dim, embedding_dim)
    def forward(self, input1):
        return self.W2(F.relu(self.W1(input1)))

def reshape_by_heads(qkv: torch.Tensor, head_num: int) -> torch.Tensor:
    batch_s, n = qkv.size(0), qkv.size(1)
    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)
    return q_reshaped.transpose(1, 2)

# ğŸ’¡ ìˆ˜ì •: multi_head_attentionì´ sparse_typeì„ ì¸ìë¡œ ë°›ë„ë¡ ë³€ê²½

def multi_head_attention(q, k, v, attention_mask=None, sparse_type=None):
    batch_s, head_num, n, key_dim = q.shape
    score = torch.matmul(q, k.transpose(2, 3))
    score_scaled = score / (key_dim ** 0.5)
    
    """"""
    # attention_maskê°€ ì œê³µë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    if attention_mask is not None:
        # attention_maskì˜ ì°¨ì›(dimension)ì„ ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.
        # Multi-Head Attentionì—ì„œëŠ” (batch, head, query_len, key_len) í˜•íƒœê°€ í•„ìš”í•©ë‹ˆë‹¤.
        if attention_mask.dim() == 3:
            # (batch, query_len, key_len) -> (batch, 1, query_len, key_len)
            attention_mask = attention_mask.unsqueeze(1)
        elif attention_mask.dim() == 2:
            # (query_len, key_len) -> (batch, 1, 1, query_len, key_len)
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        
        # attention_maskì˜ ê°’ì´ 0ì¸ ëª¨ë“  ìœ„ì¹˜ë¥¼ -infë¡œ ì±„ì›ë‹ˆë‹¤.
        score_scaled = score_scaled.masked_fill(attention_mask == 0, -1e12)


        
    if sparse_type == 'topk':
        # Top-K Sparse Attention
        # ì–´í…ì…˜ ìŠ¤ì½”ì–´ê°€ ë†’ì€ Kê°œë§Œ ì„ íƒí•˜ì—¬ ë§ˆìŠ¤í¬ ìƒì„±
        # ğŸ’¡ [í•µì‹¬ ë³€ê²½] K ê°’ì„ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ì ˆë°˜ìœ¼ë¡œ ë™ì  ê³„ì‚°
        #    k_top_k íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°í•˜ê³ , score_scaledì˜ ë§ˆì§€ë§‰ ì°¨ì› í¬ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        seq_len = score_scaled.size(-1)
        k_for_topk = max(1, seq_len // 2) # ìµœì†Œ 1ê°œë¥¼ ë³´ì¥í•˜ë©´ì„œ ì ˆë°˜ì„ ì„ íƒ

        # ì–´í…ì…˜ ìŠ¤ì½”ì–´ê°€ ë†’ì€ Kê°œë§Œ ì„ íƒí•˜ì—¬ ë§ˆìŠ¤í¬ ìƒì„±
        top_k_values, top_k_indices = torch.topk(score_scaled, k=k_for_topk, dim=-1)
        
        # ì„ íƒë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ê°’ë“¤ì€ -infë¡œ ë§ˆìŠ¤í‚¹
        topk_mask = torch.zeros_like(score_scaled, dtype=torch.bool).scatter_(-1, top_k_indices, True)
        attention_weights = score_scaled.masked_fill(~topk_mask, -1e12)
        weights = nn.Softmax(dim=3)(attention_weights)
    else:
        # Standard (Dense) Attention
        weights = nn.Softmax(dim=3)(score_scaled)
        
    out = torch.matmul(weights, v)
    out_transposed = out.transpose(1, 2)
    return out_transposed.contiguous().view(batch_s, n, head_num * key_dim)

# ğŸ’¡ ìˆ˜ì •: EncoderLayerê°€ sparse_typeì„ ì¸ìë¡œ ë°›ë„ë¡ ë³€ê²½
class EncoderLayer(nn.Module):
    def __init__(self, embedding_dim, head_num, qkv_dim, ffd='siglu', use_sparse=False, **model_params):
        super().__init__()
        self.embedding_dim, self.head_num, self.qkv_dim = embedding_dim, head_num, qkv_dim
        self.Wq, self.Wk, self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False), nn.Linear(embedding_dim, head_num * qkv_dim, bias=False), nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)
        self.normalization1 = Normalization(embedding_dim, **model_params)
        if ffd == 'siglu': self.feed_forward = ParallelGatedMLP(hidden_size=embedding_dim, **model_params)
        else: self.feed_forward = FeedForward(embedding_dim=embedding_dim, **model_params)
        self.normalization2 = Normalization(embedding_dim, **model_params)
        self.use_sparse = use_sparse

    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:
        q, k, v = reshape_by_heads(self.Wq(x), self.head_num), reshape_by_heads(self.Wk(x), self.head_num), reshape_by_heads(self.Wv(x), self.head_num)
        sparse_type = 'topk' if self.use_sparse else None
        mha_out = self.multi_head_combine(multi_head_attention(q, k, v, attention_mask=attention_mask, sparse_type=sparse_type))
        h = self.normalization1(x + mha_out)
        return self.normalization2(h + self.feed_forward(h))

class PocatPromptNet(nn.Module):
    def __init__(self, embedding_dim: int, num_nodes: int, **kwargs):
        super().__init__()
        # 1. ìŠ¤ì¹¼ë¼ ì œì•½ì¡°ê±´(4ê°œ)ì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬
        self.scalar_net = nn.Sequential(
            nn.Linear(4, embedding_dim // 2),
            nn.ReLU(),
            nn.Linear(embedding_dim // 2, embedding_dim // 2)
        )
        
        # 2. ì‹œí€€ìŠ¤ ì œì•½ í–‰ë ¬(num_nodes * num_nodes)ì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬
        self.matrix_net = nn.Sequential(
            nn.Linear(num_nodes * num_nodes, embedding_dim),
            nn.ReLU(),
            nn.Linear(embedding_dim, embedding_dim // 2)
        )
        
        # 3. ê²°í•©ëœ ì„ë² ë”©ì„ ìµœì¢… ì²˜ë¦¬í•˜ëŠ” ë„¤íŠ¸ì›Œí¬
        self.final_processor = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim), # (emb/2 + emb/2) -> emb
            nn.LayerNorm(embedding_dim),
            nn.ReLU()
        )

    def forward(self, scalar_features: torch.Tensor, matrix_features: torch.Tensor) -> torch.Tensor:
        # ê° ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼ì‹œì¼œ ì„ë² ë”© ìƒì„±
        scalar_embedding = self.scalar_net(scalar_features)
        
        # í–‰ë ¬ì„ 1ì°¨ì›ìœ¼ë¡œ í¼ì³ì„œ ì…ë ¥
        batch_size = matrix_features.shape[0]
        matrix_flat = matrix_features.view(batch_size, -1)
        matrix_embedding = self.matrix_net(matrix_flat)
        
        # ë‘ ì„ë² ë”©ì„ ì—°ê²°(concatenate)
        combined_embedding = torch.cat([scalar_embedding, matrix_embedding], dim=-1)
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì„ë² ë”© ìƒì„±
        final_prompt_embedding = self.final_processor(combined_embedding)
        
        # (batch, 1, embedding_dim) í˜•íƒœë¡œ ë¦¬í„´
        return final_prompt_embedding.unsqueeze(1)


# ğŸ’¡ ìˆ˜ì •: PocatEncoderë¥¼ CaDAì™€ ê°™ì€ ë“€ì–¼ ì–´í…ì…˜ êµ¬ì¡°ë¡œ ë³€ê²½
class PocatEncoder(nn.Module):
    def __init__(self, embedding_dim: int, encoder_layer_num: int = 6, **model_params):
        super().__init__()
        # << ìˆ˜ì •: ë‹¨ì¼ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ì œê±°í•˜ê³ , ë…¸ë“œ ìœ í˜•ë³„ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        # self.embedding_layer = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_battery = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_ic = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_load = nn.Linear(FEATURE_DIM, embedding_dim)        
        self.rail_type_embedding = nn.Embedding(3, embedding_dim)  # 0:Normal, 1:Supplier, 2:Path

        # Sparse íŒŒë¼ë¯¸í„°ë¥¼ ë³µì‚¬í•˜ì—¬ ìˆ˜ì •
        sparse_params = model_params.copy(); sparse_params['use_sparse'] = True
        global_params = model_params.copy(); global_params['use_sparse'] = False
        self.sparse_layers = nn.ModuleList([EncoderLayer(embedding_dim=embedding_dim, **sparse_params) for _ in range(encoder_layer_num)])
        self.global_layers = nn.ModuleList([EncoderLayer(embedding_dim=embedding_dim, **global_params) for _ in range(encoder_layer_num)])
        self.sparse_fusion = nn.ModuleList([nn.Linear(embedding_dim, embedding_dim) for _ in range(encoder_layer_num)])
        self.global_fusion = nn.ModuleList([nn.Linear(embedding_dim, embedding_dim) for _ in range(encoder_layer_num - 1)])

    def forward(self, td: TensorDict, prompt_embedding: torch.Tensor) -> torch.Tensor:
        node_features = td['nodes']
        batch_size, num_nodes, embedding_dim = node_features.shape[0], node_features.shape[1], self.embedding_battery.out_features
        node_embeddings = torch.zeros(batch_size, num_nodes, embedding_dim, device=node_features.device)
        
        node_type_indices = node_features[..., FEATURE_INDEX["node_type"][0]:FEATURE_INDEX["node_type"][1]].argmax(dim=-1)
        battery_mask, ic_mask, load_mask = (node_type_indices == NODE_TYPE_BATTERY), (node_type_indices == NODE_TYPE_IC), (node_type_indices == NODE_TYPE_LOAD)
        
        if battery_mask.any(): node_embeddings[battery_mask] = self.embedding_battery(node_features[battery_mask])
        if ic_mask.any(): node_embeddings[ic_mask] = self.embedding_ic(node_features[ic_mask])
        if load_mask.any(): node_embeddings[load_mask] = self.embedding_load(node_features[load_mask])

        # --- ğŸ‘‡ [í•µì‹¬] ì„ë² ë”© ì£¼ì… ë¡œì§ ---
        # 2. Load ë…¸ë“œì— ëŒ€í•´ì„œë§Œ ì¶”ê°€ì ìœ¼ë¡œ 'ë…ë¦½ ë ˆì¼' íŠ¹ì„± ì„ë² ë”©ì„ ë”í•´ì¤ë‹ˆë‹¤.
        if load_mask.any():
            # í”¼ì²˜ í…ì„œì—ì„œ ë…ë¦½ ë ˆì¼ ID(0,1,2) ì¶”ì¶œ
            rail_ids = node_features[..., FEATURE_INDEX["independent_rail_type"]].round().long().clamp(0, 2)
            
            # ë…ë¦½ ë ˆì¼ ì„ë² ë”© ê°’ì„ ê°€ì ¸ì˜´
            rail_embeds_to_add = self.rail_type_embedding(rail_ids)

            # Load ë…¸ë“œì¸ ìœ„ì¹˜ì—ë§Œ ì´ ê°’ì„ ë”í•´ì¤Œ
            # (B, N, D) í˜•íƒœì˜ í…ì„œì— (B, N) í˜•íƒœì˜ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ unsqueeze ì‚¬ìš©
            node_embeddings = node_embeddings + rail_embeds_to_add * load_mask.unsqueeze(-1).float()
        # --- ì„ë² ë”© ì£¼ì… ì™„ë£Œ ---       
        
        connectivity_mask = td['connectivity_matrix']
        global_input = torch.cat((node_embeddings, prompt_embedding), dim=1)
        global_attention_mask = torch.ones(batch_size, num_nodes + 1, num_nodes + 1, dtype=torch.bool, device=node_embeddings.device)
        global_attention_mask[:, :num_nodes, :num_nodes] = connectivity_mask
        
        sparse_out, global_out = node_embeddings, global_input
        for i in range(len(self.sparse_layers)):
            sparse_out = self.sparse_layers[i](sparse_out, attention_mask=connectivity_mask)
            global_out = self.global_layers[i](global_out, attention_mask=global_attention_mask)
            sparse_out = sparse_out + self.sparse_fusion[i](global_out[:, :num_nodes])
            if i < len(self.global_layers) - 1:
                global_nodes = global_out[:, :num_nodes] + self.global_fusion[i](sparse_out)
                global_out = torch.cat((global_nodes, global_out[:, num_nodes:]), dim=1)  
        return sparse_out


# ğŸ’¡ [CaDA ì¥ì  ì ìš© 2] ë””ì½”ë” ë¡œì§ ìˆ˜ì •
class PocatDecoder(nn.Module):
    def __init__(self, embedding_dim, head_num, qkv_dim, **model_params):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.head_num, self.qkv_dim = head_num, qkv_dim
        self.Wk, self.Wv, self.Wk_logit = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False), nn.Linear(embedding_dim, head_num * qkv_dim, bias=False), nn.Linear(embedding_dim, embedding_dim, bias=False)
        
        # ìƒíƒœ ë²¡í„° ì°¨ì›: 3 (avg_current, unconnected_ratio, step_ratio)
        self.Wq_context = nn.Linear(embedding_dim + 3, head_num * qkv_dim, bias=False)
        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)

    def forward(self, td: TensorDict, cache: PrecomputedCache):
        # ë™ì  ìƒíƒœ í”¼ì²˜ ìƒì„±
        avg_current = td["nodes"][:, :, FEATURE_INDEX["current_out"]].mean(dim=1, keepdim=True)
        unconnected_ratio = td["unconnected_loads_mask"].float().mean(dim=1, keepdim=True)
        num_nodes = td["nodes"].shape[1]
        step_ratio = td["step_count"].float() / (2 * num_nodes)

        state_features = torch.cat([avg_current, unconnected_ratio, step_ratio], dim=1)

        # Trajectory Headì˜ ì„ë² ë”©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        head_idx = td["trajectory_head"].squeeze(-1)
        head_emb = cache.node_embeddings[torch.arange(td.batch_size[0]), head_idx]
        
        query_input = torch.cat([head_emb, state_features], dim=1)
        q = reshape_by_heads(self.Wq_context(query_input.unsqueeze(1)), self.head_num)
        
        mha_out = multi_head_attention(q, cache.glimpse_key, cache.glimpse_val)
        mh_atten_out = self.multi_head_combine(mha_out)
        scores = torch.matmul(mh_atten_out, cache.logit_key).squeeze(1) / (self.embedding_dim ** 0.5)
        return scores

class PocatModel(nn.Module):
    def __init__(self, **model_params):
        super().__init__()
        self.logit_clipping = model_params.get('logit_clipping', 10)

        self.prompt_net = PocatPromptNet(embedding_dim=model_params['embedding_dim'], num_nodes=model_params['num_nodes'])
        self.encoder = PocatEncoder(**model_params)
        self.decoder = PocatDecoder(**model_params)
        # ğŸ’¡ [CaDA ì¥ì  ì ìš© 4] GRUCell ì œê±° (ìƒíƒœ ê¸°ë°˜ ì¿¼ë¦¬ë¡œ ëŒ€ì²´)
        # self.context_gru = nn.GRUCell(model_params['embedding_dim'] * 2, model_params['embedding_dim'])

    def forward(self, td: TensorDict, env: PocatEnv, decode_type: str = 'greedy', pbar: object = None,
                status_msg: str = "", log_fn=None, log_idx: int = 0, log_mode: str = 'progress'):
        base_desc = pbar.desc.split(' | ')[0] if pbar else ""
        
        if pbar:
            desc = f"{base_desc} | {status_msg} | â–¶ Encoding (ing..)"
            pbar.set_description(desc)
            if log_fn and log_mode == 'detail': log_fn(desc)
        
        # 1. ì¸ì½”ë”©
        prompt_embedding = self.prompt_net(td["scalar_prompt_features"], td["matrix_prompt_features"])
        encoded_nodes = self.encoder(td, prompt_embedding)        

        # ğŸ’¡ [CaDA ì¥ì  ì ìš© 5] ë””ì½”ë”© ì‹œì‘ ì „ Key, Value ì‚¬ì „ ê³„ì‚° ë° ìºì‹±
        # ë””ì½”ë”ì—ì„œ ì‚¬ìš©í•  Key, Valueë¥¼ ë¯¸ë¦¬ ê³„ì‚°
        glimpse_key = reshape_by_heads(self.decoder.Wk(encoded_nodes), self.decoder.head_num)
        glimpse_val = reshape_by_heads(self.decoder.Wv(encoded_nodes), self.decoder.head_num)
        logit_key = encoded_nodes.transpose(1, 2) # Single-head attentionìš©
        
        cache = PrecomputedCache(encoded_nodes, glimpse_key, glimpse_val, logit_key)

        # 2. ë””ì½”ë”© ì¤€ë¹„ (POMO)
        num_starts, start_nodes_idx = env.select_start_nodes(td)
        node_names = env.generator.config.node_names
        num_total_loads = env.generator.num_loads
        
        batch_size = td.batch_size[0]
        
        td_expanded_view = batchify(td, num_starts)
        td = td_expanded_view.clone()
        # ìºì‹œë„ POMOì— ë§ê²Œ í™•ì¥
        cache = cache.batchify(num_starts)

        # POMO ì‹œì‘: ì²« ì•¡ì…˜ì„ ê°ê¸° ë‹¤ë¥¸ Loadë¡œ ì„¤ì •
        action = start_nodes_idx.repeat(batch_size).unsqueeze(-1)

        
        td.set("action", action)
        output_td = env.step(td)
        td = output_td["next"]

        log_probs, actions = [torch.zeros(td.batch_size[0], device=td.device)], [action]


        decoding_step = 0
        while not td["done"].all():
            decoding_step += 1
            
            scores = self.decoder(td, cache)
            # tanh í•¨ìˆ˜ë¥¼ ì´ìš©í•´ scoreë¥¼ -1ê³¼ 1 ì‚¬ì´ë¡œ ì••ì¶•í•˜ê³ ,
            # clipping ê°’(10)ì„ ê³±í•´ ìµœì¢… scoreê°€ -10ê³¼ 10 ì‚¬ì´ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì œí•œí•©ë‹ˆë‹¤.
            scores = self.logit_clipping * torch.tanh(scores)
            # --- ğŸ’¡ [ìˆ˜ì •] log_mode == 'detail'ì¼ ë•Œ ë””ë²„ê·¸ ëª¨ë“œë¡œ ë§ˆìŠ¤í¬ì™€ ì´ìœ ë¥¼ í•¨ê»˜ ê°€ì ¸ì˜´ ---
            mask_info = None
            if log_mode == 'detail' and log_fn:
                mask_info = env.get_action_mask(td, debug=True)
                mask = mask_info["mask"]
            else:
                mask = env.get_action_mask(td)
            # --- ìˆ˜ì • ì™„ë£Œ ---
            # log_modeì— ë”°ë¼ ë‹¤ë¥¸ ë¡œê·¸ ì¶œë ¥
            if log_mode == 'detail' and log_fn:
                # ì•ˆì „ì¥ì¹˜: log_idxê°€ ë°°ì¹˜ í¬ê¸°ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í™•ì¸
                if log_idx >= td.batch_size[0]:
                    log_idx = 0

                head_idx = td["trajectory_head"][log_idx].item()
                head_name = node_names[head_idx]
                
                # ğŸ’¡ [ìˆ˜ì •] log_idxì˜ POMO ì‹œì‘ ë…¸ë“œ ì´ë¦„ í‘œì‹œ
                pomo_start_node_idx = start_nodes_idx[log_idx % num_starts].item()
                pomo_start_node_name = node_names[pomo_start_node_idx]
                log_msg = f"--- [Log Instance {log_idx} (Start: {pomo_start_node_name})] Step {decoding_step}: "

                if head_idx == BATTERY_NODE_IDX:
                    log_msg += f"Head is at '{head_name}'. Action Type: [Select New Load]"
                else:
                    log_msg += f"Head is at '{head_name}'. Action Type: [Find Parent]"
                log_fn(log_msg)

                num_valid_actions = mask[log_idx].sum().item()
                log_fn(f"    - Valid actions before masking: {num_valid_actions}")

                instance_scores = scores[log_idx].clone()

                valid_node_indices = torch.where(mask[log_idx])[0]
                valid_scores = instance_scores[mask[log_idx]]

                # --- ğŸ’¡ [ì¶”ê°€] debug_env.py ìŠ¤íƒ€ì¼ë¡œ ë§ˆìŠ¤í‚¹ ì´ìœ  ì¶œë ¥ ---
                if mask_info and mask_info["reasons"]:

                    reason_keys = list(mask_info["reasons"].keys())
                    reasons = mask_info["reasons"]
                    
                    if not reason_keys:
                        log_fn("    - (No masking reasons returned by environment)")
                    elif "Not Load" in reason_keys: # [Find Parent] ëª¨ë“œ
                        # ğŸ’¡ [í•µì‹¬ ìˆ˜ì •] "Find Parent" ëª¨ë“œì¼ ë•Œ "Unconnected Load" í‚¤ê°€ ì„ì—¬ìˆìœ¼ë©´ ì œê±°
                        if "Unconnected Load" in reason_keys:
                            reason_keys.remove("Unconnected Load")
                        # ğŸ’¡ [í•µì‹¬ ìˆ˜ì •] ì „ì—­ log_idxë¥¼ ì§€ì—­ local_idxë¡œ ë³€í™˜
                        current_head = td["trajectory_head"].squeeze(-1)
                        head_is_battery = (current_head == BATTERY_NODE_IDX)
                        b_idx_node = torch.where(~head_is_battery)[0] # "Find Parent" ëª¨ë“œì¸ ì „ì—­ ì¸ë±ìŠ¤ ëª©ë¡
                        
                        local_idx_matches = (b_idx_node == log_idx).nonzero()
                        
                        if local_idx_matches.numel() > 0:
                            local_idx = local_idx_matches[0, 0].item() # reasons í…ì„œì—ì„œ ì½ì–´ì˜¬ ì‹¤ì œ í–‰(row)
                            


                            header = f"{'Node Name':<50} | {'VALID?':<8} | " + " | ".join(f"{k:<10}" for k in reason_keys)
                            log_fn("\n    --- Masking Details (Mode: Find Parent) ---")
                            log_fn(header)
                            log_fn("-" * len(header))

                            for node_idx, node_name in enumerate(node_names):
                                is_valid = mask[log_idx, node_idx].item()
                                reason_str_parts = []
                                for k in reason_keys:
                                    tensor = reasons[k]
                                    value = tensor[node_idx] if tensor.ndim == 1 else tensor[local_idx, node_idx]
                                    reason_str_parts.append(f"{('âœ…' if value else 'âŒ'):<10}")
                                reason_str = " | ".join(reason_str_parts)                                
                                log_fn(f"{node_name:<50} | {('âœ… YES' if is_valid else 'âŒ NO'):<8} | {reason_str}")
                        else:
                            log_fn(f"    - (Error: Log instance {log_idx} not found in 'Find Parent' batch)")
                    
                    elif "Unconnected Load" in reason_keys: # [Select New Load] ëª¨ë“œ
                        reasons_for_instance = {k: v[log_idx] for k, v in reasons.items() if v.ndim == 2 and v.shape[0] > log_idx}                        
                        log_fn("\n    --- Masking Details (Mode: Select New Load) ---")
                        log_fn(f"{'Node Name':<50} | {'VALID?':<8} | Unconnected Load")
                        log_fn("-" * 79)
                        for node_idx, node_name in enumerate(node_names):
                            is_valid = mask[log_idx, node_idx].item()
                            if "Unconnected Load" in reasons_for_instance and reasons_for_instance["Unconnected Load"][node_idx].item():
                                log_fn(f"{node_name:<50} | {('âœ… YES' if is_valid else 'âŒ NO'):<8} | {'âœ…' if is_unconnected else 'âŒ'}")
                # --- ë§ˆìŠ¤í‚¹ ì´ìœ  ì¶œë ¥ ì™„ë£Œ ---

                if len(valid_scores) > 0:
                    # Softmax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                    valid_probs = F.softmax(valid_scores, dim=0)

                    # ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•©ë‹ˆë‹¤.
                    sorted_indices = torch.argsort(valid_scores, descending=True)
                    
                    log_fn("\n    - Top Valid Action Probabilities (for Log Instance):")
                    # ì •ë ¬ëœ ìˆœì„œëŒ€ë¡œ ëª¨ë“  ìœ íš¨ ì•¡ì…˜ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
                    for i, sorted_idx in enumerate(sorted_indices):
                        node_idx = valid_node_indices[sorted_idx].item()
                        prob = valid_probs[sorted_idx].item()
                        node_name = node_names[node_idx]
                        log_fn(f"        - {node_name:<40s} | Probability: {prob:.2%}")
                        # ìƒìœ„ 5ê°œê¹Œì§€ë§Œ ì¶œë ¥
                        if i >= 4:
                            log_fn(f"        - ... (and {len(sorted_indices) - 5} more)")
                            break 
                else:
                    log_fn("    - âŒ No valid actions found for this instance!")

            elif log_mode == 'progress' and pbar:
                unconnected_loads = td['unconnected_loads_mask'][0].sum().item()
                connected_loads = num_total_loads - unconnected_loads
                progress_msg = f"Connecting Loads ({connected_loads}/{num_total_loads})"
                desc = f"{base_desc} | {status_msg} | {progress_msg}"
                pbar.set_description(desc)

            scores.masked_fill_(~mask, -float('inf'))

            # --- ğŸ‘‡ ì—¬ê¸°ë¶€í„° ìˆ˜ì •ëœ ì½”ë“œì…ë‹ˆë‹¤ ---
            # ëª¨ë“  scoreê°€ -infê°€ ë˜ì–´ë²„ë¦¬ëŠ” 'ë§‰ë‹¤ë¥¸ ê¸¸' ìƒí™©ì„ ê°ì§€í•©ë‹ˆë‹¤.
            is_stuck = torch.all(scores == -float('inf'), dim=-1)
            
            # ë§‰ë‹¤ë¥¸ ê¸¸ì— ë„ë‹¬í•œ ë°°ì¹˜ê°€ ìˆë‹¤ë©´,
            if is_stuck.any():
                # í•´ë‹¹ ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ í–‰ë™(action 0)ì˜ scoreë¥¼ 0.0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
                # ì´ë ‡ê²Œ í•˜ë©´ log_softmax ê²°ê³¼ê°€ [0.0, -inf, -inf, ...]ê°€ ë˜ê³ ,
                # ìµœì¢… í™•ë¥ (probs)ì€ [1.0, 0.0, 0.0, ...]ì´ ë˜ì–´ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                scores[is_stuck, 0] = 0.0
            # --- ğŸ‘† ì—¬ê¸°ê¹Œì§€ ìˆ˜ì •ëœ ì½”ë“œì…ë‹ˆë‹¤ ---
            
            log_prob = F.log_softmax(scores, dim=-1)
            probs = log_prob.exp()

            action = probs.argmax(dim=-1) if decode_type == 'greedy' else Categorical(probs=probs).sample()

            if log_mode == 'detail' and log_fn:
                action_idx_log = action[log_idx].item()
                action_name = node_names[action_idx_log]
                action_prob = probs[log_idx, action_idx_log].item()
                log_fn(f"    - Action Selected: '{action_name}' (Prob: {action_prob:.2%})")
                log_fn("-" * 20)

            td.set("action", action.unsqueeze(-1))
            output_td = env.step(td)
            td = output_td["next"]
            
            actions.append(action.unsqueeze(-1))
            log_probs.append(log_prob.gather(1, action.unsqueeze(-1)).squeeze(-1))

        return {
            "reward": output_td["reward"],
            "log_likelihood": torch.stack(log_probs, 1).sum(1),
            "actions": torch.stack(actions, 1)
        }