# transformer_solver/trainer.py
import torch
from tqdm import tqdm
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
import torch.nn.functional as F # ğŸ‘ˆ F.mse_lossë¥¼ ìœ„í•´ ì¶”ê°€
import os
import time # ğŸ’¡ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ time ëª¨ë“ˆ ì¶”ê°€
from datetime import datetime
from collections import defaultdict # ğŸ‘ˆ ì´ ì¤„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.

import json
import logging

from .utils.common import TimeEstimator, clip_grad_norms, unbatchify, batchify
from .model import PocatModel, PrecomputedCache, reshape_by_heads
from .solver_env import PocatEnv

from common.data_classes import Battery, LDO, BuckConverter, Load
from .definitions import PocatConfig, NODE_TYPE_IC, FEATURE_INDEX
from .solver_env import BATTERY_NODE_IDX
from graphviz import Digraph

from .expert_dataset import ExpertReplayDataset, expert_collate_fn
from torch.utils.data import DataLoader


def update_progress(pbar, metrics):
    if pbar is None:
        return
    pbar.set_postfix({
        "Loss": f"{metrics['Loss']:.4f}",
        "Avg Cost": f"${metrics['Avg Cost']:.2f}",
        "Min Cost": f"${metrics['Min Cost']:.2f}",
        "T_Reset": f"{metrics['T_Reset']:.0f}ms",
    }, refresh=False)
    pbar.update(1)


def cal_model_size(model, log_func):
    param_count = sum(param.nelement() for param in model.parameters())
    buffer_count = sum(buffer.nelement() for buffer in model.buffers())
    log_func(f'Total number of parameters: {param_count}')
    log_func(f'Total number of buffer elements: {buffer_count}')

class PocatTrainer:
    # ğŸ’¡ 1. ìƒì„±ìì—ì„œ device ì¸ìë¥¼ ë°›ë„ë¡ ìˆ˜ì •
    def __init__(self, args, env: PocatEnv, device: str):
        self.args = args
        self.env = env
        # --- ğŸ‘‡ [DDP] DDP ê´€ë ¨ í”Œë˜ê·¸ ì €ì¥ ---
        self.is_ddp = args.ddp
        self.local_rank = args.local_rank
        # --- ğŸ‘† [DDP] ìˆ˜ì • ì™„ë£Œ ---
        self.device = device # ì „ë‹¬ë°›ì€ device ì €ì¥

        self.result_dir = args.result_dir

        
        # ğŸ’¡ 2. CUDA ê°•ì œ ì„¤ì • ë¼ì¸ ì‚­ì œ
        # torch.set_default_tensor_type('torch.cuda.FloatTensor') 
        
        # ğŸ’¡ 3. ëª¨ë¸ì„ ìƒì„± í›„, ì§€ì •ëœ deviceë¡œ ì´ë™
        self.model = PocatModel(**args.model_params).to(self.device)
        # --- ğŸ‘‡ [DDP] ëª¨ë¸ì„ DDPë¡œ ê°ì‹¸ê¸° ---
        if self.is_ddp:
            # find_unused_parameters=TrueëŠ” ë³µì¡í•œ ëª¨ë¸ì—ì„œ ì¼ë¶€ íŒŒë¼ë¯¸í„°ê°€ ì‚¬ìš©ë˜ì§€ ì•Šì„ ë•Œ ë™ê¸°í™” ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
            self.model = DDP(self.model, device_ids=[self.local_rank], find_unused_parameters=True)
        # --- ğŸ‘† [DDP] ìˆ˜ì • ì™„ë£Œ ---
        cal_model_size(self.model, args.log)
        
        # ğŸ’¡ float()ìœ¼ë¡œ ê°ì‹¸ì„œ ê°’ì„ ìˆ«ìë¡œ ê°•ì œ ë³€í™˜í•©ë‹ˆë‹¤.
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=float(args.optimizer_params['optimizer']['lr']),
            weight_decay=float(args.optimizer_params['optimizer'].get('weight_decay', 0)),
        )
        
        if args.optimizer_params['scheduler']['name'] == 'MultiStepLR':
            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=args.optimizer_params['scheduler']['milestones'],
                gamma=args.optimizer_params['scheduler']['gamma']
            )
        else:
            raise NotImplementedError
            
        self.start_epoch = 1

        # ğŸ’¡ ëª¨ë¸ ë¡œë”© ë¡œì§ ì¶”ê°€
        if args.load_path is not None:
            args.log(f"Loading model checkpoint from: {args.load_path}")
            checkpoint = torch.load(args.load_path, map_location=device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            # í›ˆë ¨ì„ ì´ì–´ì„œ í•  ê²½ìš° optimizer ìƒíƒœë„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŒ
            # self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            # self.start_epoch = checkpoint['epoch'] + 1        
        self.time_estimator = TimeEstimator(log_fn=args.log)

        self.eval_batch_size = getattr(args, "eval_batch_size", 128)
        with torch.no_grad():
            self._eval_td_fixed = self.env.reset(batch_size=self.eval_batch_size).clone()
        self.best_eval_bom = float("inf")

    # --- ğŸ‘‡ [ì‹ ê·œ] Critic ì‚¬ì „í›ˆë ¨ í•¨ìˆ˜ ---
    def pretrain_critic(self, expert_data_path: str, pretrain_epochs: int = 5, pretrain_batch_size: int = 64):
        """
        'ì •ë‹µì§€' ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ A2C ëª¨ë¸ì˜ Critic(value_head)ë§Œ ì‚¬ì „í›ˆë ¨í•©ë‹ˆë‹¤.
        """
        args = self.args
        args.log("=================================================================")
        args.log(f"ğŸ§  Critic ì‚¬ì „í›ˆë ¨(Pre-training) ì‹œì‘...")
        args.log(f"   - ì •ë‹µì§€ ê²½ë¡œ: {expert_data_path}")
        args.log(f"   - ì—í¬í¬: {pretrain_epochs}, ë°°ì¹˜ í¬ê¸°: {pretrain_batch_size}")

        # 1. 'ì •ë‹µì§€ ë¦¬í”Œë ˆì´' ë°ì´í„°ì…‹ ë¡œë“œ
        try:
            expert_dataset = ExpertReplayDataset(expert_data_path, self.env, self.device)
            if len(expert_dataset) == 0:
                args.log("âŒ ì˜¤ë¥˜: 'ì •ë‹µì§€' ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆì–´ ì‚¬ì „í›ˆë ¨ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            expert_loader = DataLoader(
                expert_dataset,
                batch_size=pretrain_batch_size,
                shuffle=True, # ì§€ë„í•™ìŠµì´ë¯€ë¡œ ì…”í”Œë§
                num_workers=0, # (í™˜ê²½ ê°ì²´ ì§ë ¬í™” ë¬¸ì œë¡œ 0 ê¶Œì¥)
                collate_fn=expert_collate_fn # ğŸ‘ˆ [ìˆ˜ì •] ì»¤ìŠ¤í…€ collate í•¨ìˆ˜ ì§€ì •
            )
        except Exception as e:
            args.log(f"âŒ ì˜¤ë¥˜: 'ì •ë‹µì§€' ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return

        # 2. Critic íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ëŠ” ë³„ë„ì˜ ì˜µí‹°ë§ˆì´ì € ìƒì„±
        critic_params = list(self.model.decoder.value_head.parameters())
        # (ì„ íƒ) Criticì´ ì‚¬ìš©í•˜ëŠ” ê³µí†µ ë ˆì´ì–´(MHA)ë„ í•¨ê»˜ í•™ìŠµ
        # critic_params += list(self.model.decoder.Wq_context.parameters())
        # critic_params += list(self.model.decoder.multi_head_combine.parameters())
        
        critic_optimizer = torch.optim.AdamW(
            critic_params,
            lr=float(args.optimizer_params['optimizer']['lr']) # A2Cì™€ ë™ì¼í•œ LR ì‚¬ìš©
        )

        self.model.train() # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ

        for epoch in range(1, pretrain_epochs + 1):
            pbar = tqdm(expert_loader, desc=f"Critic Pre-train Epoch {epoch}/{pretrain_epochs}", dynamic_ncols=True)
            total_v_loss = 0
            
            for state_td_batch, target_reward_batch in pbar:
                critic_optimizer.zero_grad()
                # --- ğŸ‘‡ [BUG FIX] ---
                # DataLoaderê°€ (B, 1, ...)ë¡œ ë¬¶ì–´ì¤€ ë°°ì¹˜ì—ì„œ ë¶ˆí•„ìš”í•œ 1ì°¨ì›ì„ ì œê±°
                state_td_batch = state_td_batch.squeeze(1)
                # target_reward_batchëŠ” (B, 1)ì´ë¯€ë¡œ squeeze ë¶ˆí•„ìš”
                # 3. ëª¨ë¸ì˜ ì¸ì½”ë”/ë””ì½”ë” ë¡œì§ ì‹¤í–‰ (model.forward()ì™€ ìœ ì‚¬)

                prompt_embedding = self.model.prompt_net(state_td_batch["scalar_prompt_features"], state_td_batch["matrix_prompt_features"])
                encoded_nodes = self.model.encoder(state_td_batch, prompt_embedding)
                
                glimpse_key = reshape_by_heads(self.model.decoder.Wk(encoded_nodes), self.model.decoder.head_num)
                glimpse_val = reshape_by_heads(self.model.decoder.Wv(encoded_nodes), self.model.decoder.head_num)
                logit_key = encoded_nodes.transpose(1, 2)
                cache = PrecomputedCache(encoded_nodes, glimpse_key, glimpse_val, logit_key)
                
                # 4. Criticì˜ ê°€ì¹˜ ì˜ˆì¸¡ (Actorì˜ scoresëŠ” ë¬´ì‹œ)
                _ , predicted_value = self.model.decoder(state_td_batch, cache) # (B, 1)
                
                # 5. [í•µì‹¬] V_Loss ê³„ì‚°: Criticì˜ ì˜ˆì¸¡ vs "ì •ë‹µì§€"ì˜ ì‹¤ì œ ë³´ìƒ
                # target_reward_batchëŠ” (B, 1), predicted_valueëŠ” (B, 1)
                critic_loss = F.mse_loss(predicted_value, target_reward_batch)
                
                # 6. Critic íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸
                critic_loss.backward()
                critic_optimizer.step()
                
                total_v_loss += critic_loss.item()
                pbar.set_postfix({"V_Loss (Pre)": f"{critic_loss.item():.4f}"})

            args.log(f"Critic Pre-train Epoch {epoch} | Avg V_Loss: {total_v_loss / len(expert_loader):.4f}")

        args.log("âœ… Critic ì‚¬ì „í›ˆë ¨ ì™„ë£Œ.")
        args.log("=================================================================")
    # --- [ì‹ ê·œ] í•¨ìˆ˜ ì¢…ë£Œ ---





    def run(self):
        args = self.args
        self.time_estimator.reset(self.start_epoch)
        
        if args.test_only:
            self.test()
            return

        for epoch in range(self.start_epoch, args.trainer_params['epochs'] + 1):
            args.log('=================================================================')
            
            self.model.train()
            
            total_steps = args.trainer_params['train_step']
            train_pbar = tqdm(
                total=total_steps,
                desc=f"Epoch {epoch}",
                dynamic_ncols=True,
                leave=False,
                miniters=1,
                mininterval=0.1,
                smoothing=0.1,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}",
            )
            
            total_loss = 0.0
            total_cost = 0.0
            total_policy_loss = 0.0 # ğŸ‘ˆ [A2C ë¡œê¹…] P_loss ëˆ„ì  ë³€ìˆ˜
            total_critic_loss = 0.0 # ğŸ‘ˆ [A2C ë¡œê¹…] V_loss ëˆ„ì  ë³€ìˆ˜
            min_epoch_cost = float('inf') # ğŸ’¡ **[ë³€ê²½ 1]** ì—í¬í¬ ë‚´ ìµœì†Œ ë¹„ìš©ì„ ê¸°ë¡í•  ë³€ìˆ˜ ì¶”ê°€

            for step in range(1, total_steps + 1):
                self.optimizer.zero_grad()
                
                reset_start_time = time.time()
                td = self.env.reset(batch_size=args.batch_size)
                reset_time = time.time() - reset_start_time
                
                # --- ğŸ‘‡ [í•µì‹¬ ìˆ˜ì • 1] í•™ìŠµ ì‹œ ë°ì´í„° í™•ì¥ ---
                if args.num_pomo_samples > 1:
                    # ğŸ’¡ [DDP] DDP ì‚¬ìš© ì‹œ, ì´ë¯¸ ë°°ì¹˜ê°€ world_sizeë§Œí¼ ë‚˜ë‰˜ì—ˆìœ¼ë¯€ë¡œ
                   # ê° í”„ë¡œì„¸ìŠ¤ê°€ POMO ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•˜ë©´ ì´ ë°°ì¹˜ëŠ”
                    # (B * N_pomo * World_Size)ê°€ ë©ë‹ˆë‹¤ 
                    td = batchify(td, args.num_pomo_samples)
                # --- ìˆ˜ì • ì™„ë£Œ ---


                model_start_time = time.time()
                # --- ğŸ‘‡ [í•µì‹¬] log í•¨ìˆ˜ë¥¼ ëª¨ë¸ì— ì „ë‹¬ ---
                out = self.model(td, self.env, decode_type='sampling', pbar=train_pbar,
                                     status_msg=None, log_fn=args.log,
                                     log_idx=args.log_idx, log_mode=args.log_mode)
                model_time = time.time() - model_start_time
                
                bwd_start_time = time.time()
                num_starts = self.env.generator.num_loads
                reward = out["reward"].view(-1, num_starts)
                # --- ğŸ‘‡ [A2C] Loss ê³„ì‚° ë¡œì§ ë³€ê²½ ---
                log_likelihood = out["log_likelihood"].view(-1, num_starts) # (B, N_pomo)
                value = out["value"].view(-1, num_starts) # (B, N_pomo)

                # 1. Critic Loss (Value Headê°€ ì‹¤ì œ ì´ ë³´ìƒ Gë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ)
                # (valueëŠ” V(s_1)ì„, rewardëŠ” G_1ì„ ì˜ë¯¸)
                critic_loss = F.mse_loss(value, reward)

                # 2. Policy Loss (Actor)
                # Baselineìœ¼ë¡œ reward.mean() ëŒ€ì‹  criticì˜ valueë¥¼ ì‚¬ìš©
                advantage = reward - value.detach() # .detach()ë¡œ Criticë§ì— ê·¸ë˜ë””ì–¸íŠ¸ ì „íŒŒ ì°¨ë‹¨
                policy_loss = -(advantage * log_likelihood).mean()

                # 3. Total Loss (Actor Loss + Critic Loss)
                loss = policy_loss + 0.5 * critic_loss # 0.5ëŠ” critic loss ê°€ì¤‘ì¹˜
                # --- ğŸ‘† [A2C] ìˆ˜ì • ì™„ë£Œ ---
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì˜µì…˜)
                max_norm = float(self.args.optimizer_params.get('max_grad_norm', 0))
                if max_norm > 0:
                    clip_grad_norms(self.optimizer.param_groups, max_norm=max_norm)

                # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                self.optimizer.step()

                bwd_time = time.time() - bwd_start_time

                # --- ğŸ‘‡ [DDP] 0ë²ˆ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ ---
                if self.local_rank <= 0:
                    logging.debug(
                        "Epoch %d step %d reset=%.3fms model=%.3fms backward=%.3fms",
                        epoch,
                        step,
                        reset_time * 1000,
                        model_time * 1000,
                        bwd_time * 1000,
                    )

                # ê° ìƒ˜í”Œ ì‹¤í–‰ì—ì„œ ì°¾ì€ ìµœìƒì˜ ë³´ìƒì„ ê°€ì ¸ì˜´
                best_reward_per_sample_run = reward.max(dim=1)[0]
                # ì›ë³¸ ë°°ì¹˜ ì¸ìŠ¤í„´ìŠ¤ë³„ë¡œ ê²°ê³¼ë¥¼ ì¬êµ¬ì„±
                best_reward_per_sample_run = best_reward_per_sample_run.view(args.batch_size, args.num_pomo_samples)
                # ê° ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´ ìƒ˜í”Œë“¤ ê°„ì˜ í‰ê·  ìµœìƒìœ„ ë³´ìƒì„ ê³„ì‚°
                avg_of_bests = best_reward_per_sample_run.mean(dim=1)

                
                # ğŸ’¡ **[ë³€ê²½ 2]** í˜„ì¬ ë°°ì¹˜ì˜ í‰ê·  ë¹„ìš©ê³¼ ìµœì†Œ ë¹„ìš© ê³„ì‚°
                avg_cost = -avg_of_bests.mean().item()
                # 'reward' í…ì„œ(ëª¨ë“  ìƒ˜í”Œ/ì‹œì‘ë…¸ë“œì˜ ë³´ìƒ)ì—ì„œ ê°€ì¥ ë†’ì€ ë³´ìƒ(=ê°€ì¥ ë‚®ì€ ë¹„ìš©)ì„ ì°¾ìŠµë‹ˆë‹¤.
                min_batch_cost = -reward.max().item()
                min_epoch_cost = min(min_epoch_cost, min_batch_cost)


                total_loss += loss.item()
                total_cost += avg_cost
                total_policy_loss += policy_loss.item()
                total_critic_loss += critic_loss.item()

                # --- ğŸ‘‡ [DDP] 0ë²ˆ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ pbar ì—…ë°ì´íŠ¸ ---
                if self.local_rank <= 0:
                    update_progress(
                        train_pbar,
                        {
                            "Loss": loss.item(),
                            "Avg Cost": total_cost / step,
                            "Min Cost": min_epoch_cost,
                            "T_Reset": reset_time * 1000,
                        },
                    )

            train_pbar.close()

            # --- ğŸ‘‡ [DDP] 0ë²ˆ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì—í­ ìš”ì•½ ë° í‰ê°€ ì‹¤í–‰ ---
            if self.local_rank <= 0:
                epoch_summary = (
                    f"Epoch {epoch}/{args.trainer_params['epochs']} | "
                    f"Total Loss {total_loss / total_steps:.4f} | "
                    f"P_Loss {total_policy_loss / total_steps:.4f} | "
                    f"V_Loss {total_critic_loss / total_steps:.4f} | "
                    f"Min Cost ${min_epoch_cost:.2f}"
                )
                tqdm.write(epoch_summary)
                args.log(epoch_summary) # ì—í­ ì¢…ë£Œ ë©”ì‹œì§€ë„ ë¡œê·¸ì— ê¸°ë¡
                
                val = self.evaluate(epoch)
                self.args.log(f"[Eval] Epoch {epoch} | Avg BOM ${val['avg_bom']:.2f} | Min BOM ${val['min_bom']:.2f}")

            self.scheduler.step()

            if self.local_rank <= 0:
                self.time_estimator.print_est_time(epoch, args.trainer_params['epochs'])            
            # --- ğŸ‘‡ [DDP] 0ë²ˆ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ëª¨ë¸ ì €ì¥ ---
            
            if self.local_rank <= 0 and ((epoch % args.trainer_params['model_save_interval'] == 0) or (epoch == args.trainer_params['epochs'])):                
                save_path = os.path.join(args.result_dir, f'epoch-{epoch}.pth')
                args.log(f"Saving model at epoch {epoch} to {save_path}")
                self._run_test_visualization(epoch, is_best=False)
                
                # ğŸ’¡ DDPë¡œ ê°ì‹¸ì§„ ëª¨ë¸ì€ .moduleì„ í†µí•´ ì›ë³¸ ëª¨ë¸ì˜ state_dictì— ì ‘ê·¼í•©ë‹ˆë‹¤.
                model_state_dict = self.model.module.state_dict() if self.is_ddp else self.model.state_dict()

                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model_state_dict,
                    'optimizer_state_dict': self.optimizer.state_dict(),
                }, save_path)

        args.log(" *** Training Done *** ")


    # ... (test, visualize_result ë©”ì†Œë“œëŠ” ê¸°ì¡´ê³¼ ë™ì¼) ...
    @torch.no_grad()
    def evaluate(self, epoch: int):
        """Greedy decode on a fixed validation set, CSV log, and save best checkpoint by avg BOM."""
        self.model.eval()
        # --- ğŸ‘‡ [í•µì‹¬ ìˆ˜ì • 4] í‰ê°€ ì‹œ ë°ì´í„° í™•ì¥ ---
        eval_samples = self.args.test_num_pomo_samples
        td_eval = self._eval_td_fixed.clone()
        if eval_samples > 1:
            td_eval = batchify(td_eval, eval_samples)

        # Rebuild eval TD from the fixed snapshot (same instances every epoch)
        td_eval = self.env._reset(self._eval_td_fixed.clone())

        # Greedy decoding; reuse your model call signature
        out = self.model(
            td_eval, self.env, decode_type='greedy',
            pbar=None, status_msg="Eval",
            log_fn=self.args.log, log_idx=self.args.log_idx, log_mode=self.args.log_mode
        )

        # POMO starts: choose best per instance
        num_starts = self.env.generator.num_loads
        reward = out["reward"].view(num_starts, -1)
        best_reward_per_instance = reward.max(dim=0)[0]

        avg_bom = -best_reward_per_instance.mean().item()
        min_bom = -best_reward_per_instance.max().item()

        # CSV log
        import os, torch
        csv_path = os.path.join(self.result_dir, "val_log.csv")
        header = not os.path.exists(csv_path)
        with open(csv_path, "a", encoding="utf-8") as f:
            if header:
                f.write("epoch,avg_bom,min_bom,decode_type\n")
            f.write(f"{epoch},{avg_bom:.4f},{min_bom:.4f},greedy\n")

        # Save best
        if avg_bom < self.best_eval_bom:
            self.best_eval_bom = avg_bom
            save_path = os.path.join(self.result_dir, "best_cost.pth")

            # ğŸ’¡ DDPë¡œ ê°ì‹¸ì§„ ëª¨ë¸ì€ .moduleì„ í†µí•´ ì›ë³¸ ëª¨ë¸ì˜ state_dictì— ì ‘ê·¼í•©ë‹ˆë‹¤.
            model_state_dict = self.model.module.state_dict() if self.is_ddp else self.model.state_dict()

            torch.save({
                'epoch': epoch,
                'model_state_dict': model_state_dict,
                'optimizer_state_dict': self.optimizer.state_dict(),
            }, save_path)

            self.args.log(f"[Eval] âœ… Running test visualization for new best model...")
            self._run_test_visualization(epoch, is_best=True)
            self.args.log(f"[Eval] âœ… New best avg_bom=${avg_bom:.2f} (min=${min_bom:.2f}) at epoch {epoch} â†’ saved {save_path}")

        return {"avg_bom": avg_bom, "min_bom": min_bom}

    def test(self):
        self.model.eval()
        logging.info("==================== INFERENCE START ====================")

        # --- ğŸ‘‡ [í•µì‹¬] _run_test_visualization í˜¸ì¶œ ---
        # test_only ëª¨ë“œì¼ ë•ŒëŠ” ì—í¬í¬ ë²ˆí˜¸ê°€ ì—†ìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •, bestë„ ì•„ë‹˜
        self._run_test_visualization(epoch=0, is_best=False)


    @torch.no_grad()
    def _run_test_visualization(self, epoch: int, is_best: bool = False):
        """
        í…ŒìŠ¤íŠ¸ ëª¨ë“œì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•œ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ê³ 
        íŒŒì›ŒíŠ¸ë¦¬ ì‹œê°í™”(PNG)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        (ê¸°ì¡´ test ë©”ì†Œë“œì˜ ë¡œì§ì„ ì´ í•¨ìˆ˜ë¡œ ì´ë™)
        """
        self.model.eval()

        if is_best:
            log_prefix = f"[Test Viz @ Epoch {epoch} (BEST)]"
            filename_prefix = f"epoch_{epoch}_best"
        elif epoch > 0: # 5 ì—í¬í¬ ê°„ê²© ì €ì¥
            log_prefix = f"[Test Viz @ Epoch {epoch}]"
            filename_prefix = f"epoch_{epoch}"
        else: # --test_onlyë¡œ ì§ì ‘ ì‹¤í–‰
            log_prefix = "[Test Viz (Standalone)]"
            filename_prefix = "test_solution"

        self.args.log(f"{log_prefix} Running inference to generate power tree...")


        # --- ğŸ‘‡ [í•µì‹¬ ìˆ˜ì • 5] í…ŒìŠ¤íŠ¸ ì‹œ ë°ì´í„° í™•ì¥ ë° ê²°ê³¼ ì²˜ë¦¬ ---
        test_samples = self.args.test_num_pomo_samples
        td = self.env.reset(batch_size=1)
        if test_samples > 1:
            td = batchify(td, test_samples)
        
        pbar = tqdm(total=1, desc=f"Solving Power Tree (Mode: {self.args.decode_type}, Samples: {test_samples})")
        out = self.model(td, self.env, decode_type=self.args.decode_type, pbar=pbar, 
                         log_fn=self.args.log, log_idx=self.args.log_idx,
                         log_mode=self.args.log_mode)
        pbar.close()

        reward = out['reward']
        actions = out['actions']
        
        # ëª¨ë“  ìƒ˜í”Œê³¼ ì‹œì‘ ë…¸ë“œ ì¤‘ì—ì„œ ë‹¨ í•˜ë‚˜ì˜ ìµœê³  ê²°ê³¼ë¥¼ ì„ íƒ
        best_idx = reward.argmax()
        final_cost = -reward[best_idx].item()
        best_action_sequence = actions[best_idx]

        # ìµœì í•´ì˜ ì‹œì‘ ë…¸ë“œ ì •ë³´ë¥¼ ì •í™•íˆ ì°¾ê¸°
        num_starts = self.env.generator.num_loads
        _, start_nodes_idx = self.env.select_start_nodes(self.env.reset(batch_size=1))
        
        best_start_node_local_idx = best_idx % num_starts
        best_start_node_idx = start_nodes_idx[best_start_node_local_idx].item()
        best_start_node_name = self.env.generator.config.node_names[best_start_node_idx]
        print(f"Generated Power Tree (Best of {test_samples} samples, start: '{best_start_node_name}'), Cost: ${final_cost:.4f}")

        action_history = []
        # ğŸ’¡ **[BUG FIX]** ì‹œë®¬ë ˆì´ì…˜ì€ POMOë¡œ í™•ì¥ëœ ë°°ì¹˜ê°€ ì•„ë‹Œ, ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤(ë°°ì¹˜ í¬ê¸° 1)ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        td_sim = self.env.reset(batch_size=1)

        # ì²« ë²ˆì§¸ ì•¡ì…˜ì€ ì‹œì‘ ë…¸ë“œë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì´ë©°, POMO ì„¤ì •ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
        # ë”°ë¼ì„œ ëª¨ë¸ì´ ë§Œë“  ì²« *ê²°ì •*ë¶€í„° ì‹œë®¬ë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.
        td_sim.set("action", best_action_sequence[0])
        output_td = self.env.step(td_sim)
        td_sim = output_td["next"]
        
        for action_tensor in best_action_sequence[1:]:
            if td_sim["done"].all(): break
            current_head = td_sim["trajectory_head"].item()
            action_item = action_tensor.item()
            
            # ë¶€ëª¨ê°€ ìì‹ì—ê²Œ í• ë‹¹ë  ë•Œë§Œ action_historyì— ì¶”ê°€í•©ë‹ˆë‹¤.
            if current_head != BATTERY_NODE_IDX:
                action_history.append((action_item, current_head))

            td_sim.set("action", action_tensor.unsqueeze(0)) # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            output_td = self.env.step(td_sim)
            td_sim = output_td["next"]

        # --- ğŸ‘‡ [í•µì‹¬ ìˆ˜ì • 3] ì‹œê°í™” í•¨ìˆ˜ì— ì‹œì‘ ë…¸ë“œ ì´ë¦„ ì „ë‹¬ ---
        self.visualize_result(action_history, final_cost, best_start_node_name, td_sim)
        # --- ğŸ‘‡ [í•µì‹¬ ìˆ˜ì •] ì‹œê°í™” í•¨ìˆ˜ì— ìƒˆ filename_prefix ì „ë‹¬ ---
        self.visualize_result(action_history, final_cost, best_start_node_name, td_sim, filename_prefix=filename_prefix)
        self.args.log(f"{log_prefix} Power tree visualization saved.")

    # ğŸ’¡ [í•µì‹¬ ìˆ˜ì •] visualize_result ë©”ì„œë“œë¥¼ OR-Tools ìˆ˜ì¤€ìœ¼ë¡œ ëŒ€í­ ì—…ê·¸ë ˆì´ë“œ
    def visualize_result(self, action_history, final_cost, best_start_node_name, final_td, filename_prefix: str = "solution"):
        if self.result_dir is None: return
        if self.result_dir is None: return
        os.makedirs(self.result_dir, exist_ok=True)

        # 1. ì •ë³´ ì¶”ì¶œ ë° ë§µ ìƒì„±
        node_names = self.env.generator.config.node_names
        loads_map = {load['name']: load for load in self.env.generator.config.loads}
        # âš ï¸ ì‚¬ìš©ëœ ICì˜ 'íŠ¹í™”ëœ' ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ generatorì˜ ì „ì²´ ëª©ë¡ì„ ì‚¬ìš©
        candidate_ics_map = {ic['name']: ic for ic in self.env.generator.config.available_ics}
        battery_conf = self.env.generator.config.battery
        constraints = self.env.generator.config.constraints
        final_features = final_td["nodes"][0]

        
        # 2. ì‚¬ìš©ëœ ë…¸ë“œ, IC, ì—£ì§€ ì •ë³´ ì¬êµ¬ì„±
        used_ic_names = set()
        child_to_parent = {}
        parent_to_children = defaultdict(list)

        for parent_idx, child_idx in action_history:
            parent_name = node_names[parent_idx]
            child_name = node_names[child_idx]
            child_to_parent[child_name] = parent_name
            parent_to_children[parent_name].append(child_name)
            if parent_name in candidate_ics_map:
                used_ic_names.add(parent_name)

        # 3. Always-On ê²½ë¡œ ì¶”ì 
        always_on_nodes = {
            name for name, conf in loads_map.items() if conf.get("always_on_in_sleep", False)
        }
        nodes_to_process = list(always_on_nodes)
        while nodes_to_process:
            node = nodes_to_process.pop(0)
            if node in child_to_parent:
                parent = child_to_parent[node]
                if parent != battery_conf['name'] and parent not in always_on_nodes:
                    always_on_nodes.add(parent)
                    nodes_to_process.append(parent)

        supplier_nodes = set()
        path_nodes = set()
        for name, conf in loads_map.items():
            rail_type = conf.get("independent_rail_type")
            if rail_type == 'exclusive_supplier':
                supplier_nodes.add(name)
                if name in child_to_parent:
                    supplier_nodes.add(child_to_parent[name])
            elif rail_type == 'exclusive_path':
                current_node = name
                while current_node in child_to_parent:
                    path_nodes.add(current_node)
                    parent = child_to_parent[current_node]
                    path_nodes.add(parent)
                    if parent == battery_conf['name']: break
                    current_node = parent

        # 4. ì•¡í‹°ë¸Œ/ìŠ¬ë¦½ ì „ë¥˜ ë° ì „ë ¥ ê³„ì‚° (Bottom-up ë°©ì‹)
        # ğŸ’¡ [ìˆ˜ì •] ì¤‘ë³µ ì„ ì–¸ì„ ì œê±°í•˜ê³  OR-Toolsì™€ ë™ì¼í•œ ë³€ìˆ˜ëª…ìœ¼ë¡œ í†µì¼í•©ë‹ˆë‹¤.
        junction_temps, actual_i_ins_active, actual_i_outs_active = {}, {}, {}
        actual_i_ins_sleep, actual_i_outs_sleep, ic_self_consumption_sleep = {}, {}, {}
        
        # ì´ˆê¸°ê°’: ë¶€í•˜ë“¤ì˜ ì „ë¥˜ ì†Œëª¨ëŸ‰ ì„¤ì •
        active_current_draw = {name: conf["current_active"] for name, conf in loads_map.items()}
        sleep_current_draw = {name: conf["current_sleep"] for name, conf in loads_map.items()}


        processed_ics = set()
        used_ic_objects = [candidate_ics_map[name] for name in used_ic_names]

        while len(processed_ics) < len(used_ic_objects):
            progress_made = False
            for ic_conf in used_ic_objects:
                ic_name = ic_conf['name']
                if ic_name in processed_ics: continue

                children_names = parent_to_children.get(ic_name, [])
                if all(c in loads_map or c in processed_ics for c in children_names):
                    ic_obj = LDO(**ic_conf) if ic_conf['type'] == 'LDO' else BuckConverter(**ic_conf)
                    
                    # Active ì „ë¥˜ ê³„ì‚°
                    # ğŸ’¡ [ìˆ˜ì •] _active ì ‘ë¯¸ì‚¬ê°€ ë¶™ì€ ë³€ìˆ˜ëª…ì„ ì‚¬ìš©í•˜ë„ë¡ í†µì¼í•©ë‹ˆë‹¤.
                    total_i_out_active = sum(active_current_draw.get(c, 0) for c in children_names)
                    actual_i_outs_active[ic_name] = total_i_out_active
                    i_in_active = ic_obj.calculate_input_current(vin=ic_obj.vin, i_out=total_i_out_active)
                    active_current_draw[ic_name] = i_in_active
                    actual_i_ins_active[ic_name] = i_in_active


                    # Sleep ì „ë¥˜ ê³„ì‚°
                    i_in_sleep, ic_self_sleep, total_i_out_sleep = 0, 0, 0
                    parent_name = child_to_parent.get(ic_name)
                    
                    if ic_name in always_on_nodes:
                        total_i_out_sleep = sum(sleep_current_draw.get(c, 0) for c in children_names)
                        ic_self_sleep = ic_obj.operating_current
                        if isinstance(ic_obj, LDO):
                            i_in_sleep = total_i_out_sleep + ic_self_sleep
                        elif isinstance(ic_obj, BuckConverter) and ic_obj.vin > 0:
                            eff_sleep = constraints.get('sleep_efficiency_guess', 0.35)
                            p_out_sleep = ic_obj.vout * total_i_out_sleep
                            p_in_sleep = p_out_sleep / eff_sleep if p_out_sleep > 0 else 0
                            i_in_sleep = (p_in_sleep / ic_obj.vin) + ic_self_sleep
                    elif parent_name in always_on_nodes or parent_name == battery_conf['name']:
                        ic_self_sleep = ic_obj.shutdown_current if (ic_obj.shutdown_current is not None and ic_obj.shutdown_current > 0) else ic_obj.quiescent_current
                        i_in_sleep = ic_self_sleep
                    
                    actual_i_ins_sleep[ic_name] = i_in_sleep
                    actual_i_outs_sleep[ic_name] = total_i_out_sleep
                    ic_self_consumption_sleep[ic_name] = ic_self_sleep
                    sleep_current_draw[ic_name] = i_in_sleep

                    processed_ics.add(ic_name)
                    progress_made = True
            if not progress_made and len(used_ic_objects) > 0: break

        # 5. ìµœì¢… ì‹œìŠ¤í…œ ì „ì²´ ê°’ ê³„ì‚°
        primary_nodes = parent_to_children.get(battery_conf['name'], [])
        total_active_current = sum(active_current_draw.get(name, 0) for name in primary_nodes)
        total_sleep_current = sum(sleep_current_draw.get(name, 0) for name in primary_nodes)
        battery_avg_voltage = (battery_conf['voltage_min'] + battery_conf['voltage_max']) / 2
        total_active_power = battery_avg_voltage * total_active_current

        # 6. Graphviz ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
        dot = Digraph(comment=f"Power Tree - Cost ${final_cost:.4f}")
        dot.attr('node', shape='box', style='rounded,filled', fontname='Arial')
        dot.attr(rankdir='LR', label=f"Transformer Solution (Start: {best_start_node_name})\\nCost: ${final_cost:.4f}", labelloc='t')

        max_sleep_current_target = constraints.get('max_sleep_current', 0.0)
        battery_label = (f"ğŸ”‹ {battery_conf['name']}\\n\\n"
            f"Total Active Power: {total_active_power:.2f} W\\n"
            f"Total Active Current: {total_active_current * 1000:.1f} mA\\n"
            f"Target Sleep Current: <= {max_sleep_current_target * 1000000:,.1f} ÂµA\n"
            f"Total Sleep Current: {total_sleep_current * 1000000:,.1f} ÂµA")
        dot.node(battery_conf['name'], battery_label, shape='Mdiamond', color='darkgreen', fillcolor='white')

        for ic_name in used_ic_names:
            ic_conf = candidate_ics_map[ic_name]
            ic_idx = node_names.index(ic_name)
            
            i_in_active_val = actual_i_ins_active.get(ic_name, 0)
            i_out_active_val = actual_i_outs_active.get(ic_name, 0)
            i_in_sleep_val = actual_i_ins_sleep.get(ic_name, 0)
            i_out_sleep_val = actual_i_outs_sleep.get(ic_name, 0)
            i_self_sleep_val = ic_self_consumption_sleep.get(ic_name, 0)
            calculated_tj = final_features[ic_idx, FEATURE_INDEX["junction_temp"]].item()
            
            thermal_margin = ic_conf['t_junction_max'] - calculated_tj
            node_color = 'blue'
            if thermal_margin < 10: node_color = 'red'
            elif thermal_margin < 25: node_color = 'orange'
            # --- ğŸ’¡ [ìˆ˜ì •] ë…¸ë“œ ìŠ¤íƒ€ì¼ë§ ë¡œì§ ---
            node_style = 'rounded,filled'
            if ic_name not in always_on_nodes:
                node_style += ',dashed'

            fill_color = 'white'
            if ic_name in path_nodes:
                fill_color = 'lightblue'
            elif ic_name in supplier_nodes:
                fill_color = 'lightyellow'
            # --- ìˆ˜ì • ì™„ë£Œ ---
            
            label = (f"ğŸ“¦ {ic_conf['name'].split('@')[0]}\\n\\n"
                     f"Vin: {ic_conf['vin']:.2f}V, Vout: {ic_conf['vout']:.2f}V\\n"
                     f"Iin: {i_in_active_val*1000:.1f}mA (Act) | {i_in_sleep_val*1000000:,.1f}ÂµA (Slp)\\n"
                     f"Iout: {i_out_active_val*1000:.1f}mA (Act) | {i_out_sleep_val*1000000:,.1f}ÂµA (Slp)\\n"
                     f"I_self: {ic_conf['operating_current']*1000:.1f}mA (Act) | {i_self_sleep_val*1000000:,.1f}ÂµA (Slp)\\n"
                     f"Tj: {calculated_tj:.1f}Â°C (Max: {ic_conf['t_junction_max']}Â°C)\\n"
                     f"Cost: ${ic_conf['cost']:.2f}")
            dot.node(ic_name, label, color=node_color, fillcolor=fill_color, style=node_style, penwidth='3')

        for name, conf in loads_map.items():
            # --- ğŸ’¡ [ìˆ˜ì •] ë¶€í•˜ ë…¸ë“œ ìŠ¤íƒ€ì¼ë§ ë¡œì§ ---
            node_style = 'rounded,filled'
            if name not in always_on_nodes:
                node_style += ',dashed'

            fill_color = 'white'
            if name in path_nodes:
                fill_color = 'lightblue'
            elif name in supplier_nodes:
                fill_color = 'lightyellow'

            label = f"ğŸ’¡ {name}\\nActive: {conf['voltage_typical']}V | {conf['current_active']*1000:.1f}mA\\n"
            if conf['current_sleep'] > 0:
                label += f"Sleep: {conf['current_sleep'] * 1000000:,.1f}ÂµA"
            penwidth = '3' if conf.get("always_on_in_sleep", False) else '1'
            dot.node(name, label, color='dimgray', fillcolor=fill_color, style=node_style, penwidth=penwidth)

        for p_name, children in parent_to_children.items():
            for c_name in children:
                dot.edge(p_name, c_name)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_cost_{final_cost:.4f}_{timestamp}"
        output_path = os.path.join(self.result_dir, filename)
        
        try:
            dot.render(output_path, view=False, format='png', cleanup=True)
            logging.info(f"âœ… ìƒì„¸ ì‹œê°í™” ë‹¤ì´ì–´ê·¸ë¨ì„ {output_path}.png íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logging.error(f"âŒ ì‹œê°í™” ë Œë”ë§ ì‹¤íŒ¨. Graphvizê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  PATHì— ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")






